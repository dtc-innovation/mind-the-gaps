---
draft: true
title: Form prototyping methodology
image: /assets/presentations/2018-02-08-odi/slide-11.png
tags:
- workshop
---

We decided to do a survey to fill the gaps in how citizens or communities experiences.
And I guess the stories that the big data collected by local government does or does not tell.

We wrote the brief. And we had no idea how to do it an effective way. So we made some research into...

Well we started with the quality of life survey. It is a quantitative output. So with existing data collection methods, thick and thin. We have found no trace of thick dataâ€“well there is the happy city index, but no available data. There are the ward/neighbourhood officers -- qualitative data -- but it is not recorded in any way. The councillor thought it would be useful when bringing pressing issues to the mayor, by bringing evidence, numbers.

In looking the QoL, we found some weird points. It's finding out the _what_ but never the _why_.
The value in fulfilling was to uncover the _whys_, as the pathways to potential resolutions.

That's when we decided to design a survey, but padded differently, including the vocabulary and language used ("Overall, ..."). We wanted to reduce the value/precision of the anwsers by softening . Not hard/crunchy data but it's about the feelings and emotions. It's never going to be a solid sample. And it's going to be completed in real time. It's the real value. It's not suposed to be used to base a policy. It's supposed to be used to be used _with_ big data to make better policy.

One of the reason we thought a survey was a good idea, was the _replicability_. Because even though Bristol as the QoL, Bordeaux does not. Which means we need a method to design the content of questionnaire from scratch. We cannot solely rely on spotting caveats in the QoL survey.

Another point is the QoL is expensive to produce. But they have been scaling it back year after year. Which is erroding the quality of the content, mostly through smaller sample, LSOA to the ward, 100 people min to any people, 3K answers from an online portal.

QoL is not user designed. It is not the voice of the people. It is to get answers to the questions they feel will make their department look the best. Or to provide metrics that will show their work in a good light (satisfaction). Or to respond to external commissions.

So we looked at other research survey questions about quality of life and wellbeing. We figured out they were mostly national or pan-european. Not designed to be used by cities much (granularity issue). Not user designed ("Overall, on a scale from 1 to 10, how were you happy yesterday?"), not natural questions. So how do you expect natural responses? We started to reference our original community interviews as part of our initial ethnographic study. Using those two feeds, general surveys and community interviews in Bristol, we wrote some mock questions and tested them on ourselves.

It was a good exercice to play with sentence structure, informality and openness.
We were making assumptions about the needs of people in Bristol.
We needed to go back to Bristol.

Another intuition we developed was to reuse answers from previous questions to formulate questions on the online questionnaire. We discovered that Typeform provided such a feature for free. We kept thinking how to be on par, online and offline, so as data collection can be of a similar quality regardless of the medium.

We went to Bristol and took a half-open structure. We used it as a framework as a semi-structured community interview. We did around 15 interviews in a round in the centre of Bristol. We then did a retro session to transcribe these interviews. We got the full extent of what people were saying.

At that point we started to rebuild the questions. We were thinking about several key things:
- how to have a nice flow of information (in a structure which could be made relevant in a dataset -- asking 1 question, rephrasing you can do that in real life but not on a digital form)
- making the questions more focused but on issues that were relevant (the more focused we get the questions, the more comparable the responses are)
- to make it more useable, we need to ask the same questions
- thomas had more success with the variation with the 5 whys technique -- to get follow up on the nature of their response
- (other key things????)

Then we went for round 2 on the same day. In different neighbourhood. We used these new questions, designed with previous cues given by respondants. This got 10 interviews, based on the same questions. We had a retro again after transcribing. This was the point we realised that simple reframing: asking people about Bristol or their neighbourhood would completely differnet answers.

We realised we would never get the perfect survey questions. We needed to work with things as a working draft.

We started turning the responses in a data sheet. And building the online form with the second iteration (remember, it was closer to the structure of a regular form this time).

Turning in a data sheet: althought it was more rich to run a semi-structured interview (we can bend and twist to get the most out of the replies). But it was more time consumming to turn that in a structured way. I copied the way I have seen similar datasets (1 column per question). Unless you start to analyse with a computer system, with code and values assigned to words -- taking out the relevant bit -- a bridge between useful and speed. I could not evaluate random questions. I could only evaluate the questions as they were directly scripted.

We tested them the day with a Typeform, in a cafe first. The plan was to go to a location with an inside space with an internet connection. The weather was wet. The plan was to stay in a public space and ask questions. And one person outdoor.

1. how does it work when people fill independently on a computer (you fill, i go back to my seat)
2. how does it work when we sit next to the person, to be a resourceful help
3. offline, but assisted (like 2. but with a paper form)

and compare the responses.

We did 2 sessions. It was much harder indoor because we did not want to bother people. But we did a morning session at the Watershed. People were computer litterate. They still had a few problems with the UX of Typeform (going back, going to the next question). Happily surprised by the shortness of the form. Feeling good to express themselves.
We did some minor amendments to help with readability and flow.

(people seem a bit cranky about the city, even though we could not help -- but there is a wellbeing element in the sharing. That's why we wanted to keep it open. To keep the self-expression open.)

We went in the afternoon for round 2. Finding a location was difficult. We wanted to test in a deprived area of Bristol. This worked very well. Assisted online this time (option #2). But we found many accessibility of the use of the online survey. Even people who could read without difficulty, it was something which was not comfortable to them. We were surprised by the amount of people (because UX, litteracy) who could not complete the form. They would rather prefer paper form or us to type for them. The feeling of talking to somebody was probably more beneficial too.

The final thing. We could keep changing the questions forever. But it did not change too much things forever. So we can keep the questions more or less the same. We had 3 topics: time, home and transport. We asked only the 'transport'. People felt it was a transport survey rather than a wellbeing/neighbourhood
 survey. We also decided we would add these two categories back in. We would look for a different user interface to better suits the needs of respondants.
